{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "## Activity Classification\n",
    "\n",
    "This notebook serves as a record of preliminary investigations into using a neural network model for LISA. Hopefully, it serves as a useful starting point for future work - but shouldn't be treated as 100% verified!\n",
    "\n",
    "A version of this process also exists in `lisa/modeling/neural_net.py`, used to train a model on the full dataset on HPC. The resulting model is saved in `models/neural_net` - feel free to experiment with this pre-trained model.\n",
    "\n",
    "### Introduction\n",
    "We're using the tensorflow keras API, the most popular approach for neural nets: https://www.tensorflow.org/guide/keras\n",
    "\n",
    "Neural nets consist of layers of interconnected nodes (neurons) that process data and learn patterns, like a human brain. They can be used for classification, regression, and even unsupervised tasks. They are generally more complex than other models, and so require more data and computational resources. \n",
    "\n",
    "Typically (but by no means in every case) they can be more successful than other types of models, but require much more tuning to get there, and are more prone to overfitting.\n",
    "\n",
    "### Installation\n",
    "Installing tensorflow/keras is slightly more involved than other models (i.e. scikitlearn) - see `post_install.sh` for macOS-specific differences - so be prepared for this to potentially take some work. Verify your successful installation with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "\n",
    "from lisa.config import MODELS_DIR, PROCESSED_DATA_DIR\n",
    "from lisa.features import sequential_stratified_split, standard_scaler\n",
    "\n",
    "# Check packages have installed correctly\n",
    "print(\"Keras version: \", keras.__version__)\n",
    "print(\"Keras tuner version: \", kt.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and validation data below. We need to convert the data to `numpy ndarrays`, and use a label encoder to convert the categorical variables to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_parquet(PROCESSED_DATA_DIR / \"P1.parquet\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = sequential_stratified_split(\n",
    "    df, 0.8, 800, [\"ACTIVITY\"]\n",
    ")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X_train, X_val, scaler = standard_scaler(X_train, X_val)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "\n",
    "y_train = y_train.collect().to_numpy()\n",
    "y_val = y_val.collect().to_numpy()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our simple feedforward (information only flows forward; backpropagation is another type) model for 3-class classification. \n",
    "It has 3 layers: input (also a hidden layer), a second hidden layer, and output. \n",
    "\n",
    "We also have 3 hyperparameters specified; `neurons1` and `neurons2` determine the number of neurons in the first 2 layers, and `dropout` determines the fraction of neurons to randomly drop between layers (to prevent overfitting).\n",
    "\n",
    "To run through the other details:\n",
    " - ReLU (Rectified Linear Unit): Activation function to introduce non-linearity.\n",
    " - Softmax: Activation function which outputs probabilities for each class. The class with the highest probability is the predicted class.\n",
    " - Adam: Adaptive optimization algorithm that adjusts learning rates during training for faster convergence.\n",
    " - Sparse Categorical Cross Entropy: Loss function, suitable for multi-class classification when the target labels are integers (not one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(neurons1, neurons2, dropout):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(neurons1, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        keras.layers.Dropout(dropout),\n",
    "        keras.layers.Dense(neurons2, activation='relu'),\n",
    "        keras.layers.Dropout(dropout),\n",
    "        keras.layers.Dense(3, activation='softmax')  # 3 classes for classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just define a function specifying the hyperparameter value ranges we want to sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(hp):\n",
    "    neurons1 = hp.Int(\"units1\", min_value=32, max_value=512, step=32)\n",
    "    neurons2 = hp.Int(\"units2\", min_value=32, max_value=512, step=32)\n",
    "    dropout = hp.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1)\n",
    "\n",
    "    model = create_classifier(\n",
    "        neurons1=neurons1, neurons2=neurons2, dropout=dropout\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the tuner used for hyperparameter optimisation. \n",
    "\n",
    "We'll use a typical Bayesian approach. \n",
    "`max_trials` specifies the number of trials we want to run (and so will impact time taken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    build_classifier,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call `search()` to train the model and and find the best hyperparameters. \n",
    "You should get a nice output of the progress and current results.\n",
    "\n",
    "`Epochs` and `batch_size` are also important parameters:\n",
    "- `Epochs`: Number of full passes through the training set.\n",
    "- `batch_size`: Number of training samples processed before updating model weights.\n",
    "\n",
    "They will affect performance, runtime, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the best model and hyperparameters, and take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(best_hps.get('units1'))\n",
    "print(best_hps.get('units2'))\n",
    "print(best_hps.get('dropout'))\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can [save the trained model](https://www.tensorflow.org/guide/keras/serialization_and_saving). Save to a `keras` file rather than `pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(MODELS_DIR / \"best_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Regression\n",
    "The process for training a regression model appears to be fairly similar. \n",
    "\n",
    "The key differences is the single linear output for the final layer, and the loss and metrics functions in the compilation step. <br>We'll forgo the hyperparameter tuning and model saving this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regression model\n",
    "regression_model = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation='linear')  # Single output for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "regression_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',  # Mean Squared Error for regression\n",
    "    metrics=['mae']  # Mean Absolute Error as an evaluation metric\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "regression_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data for regression\n",
    "X_train, X_val, y_train, y_val = sequential_stratified_split(\n",
    "    df, 0.8, 800, [\"SPEED\"]\n",
    ")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "X_train, X_val, scaler = standard_scaler(X_train, X_val)\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "\n",
    "y_train = y_train.collect().to_numpy()\n",
    "y_val = y_val.collect().to_numpy()\n",
    "\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the regression model\n",
    "history = regression_model.fit(\n",
    "    X_train, y_train,  # X_train: features, y_train: continuous target values\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = regression_model.evaluate(X_val, y_val)\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
